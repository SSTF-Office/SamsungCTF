<Papers><Paper published="yes"><Idx>1</Idx><Title>Enhancing Differential Privacy for Federated Learning at Scale</Title><Author>Chunghun Baek (Samsung Reseasrch)</Author><Conference>IEEE ACCESS</Conference><Year>2021</Year><Abstract>Federated learning (FL) is an emerging technique that trains machine learning models across multiple de-centralized systems. It enables local devices to collaboratively learn a model by aggregating locally computed updates via a server. Privacy is a core aspect of FL, and recent works in this area are advancing the privacy guarantee of an FL network. To ensure rigorous privacy guarantee for FL, prior works have focused on methods to securely aggregate local updates and provide differential privacy (DP). In this paper, we investigate a new privacy risk for FL. Specifically, FL may frequently encounter unexpected user dropouts because it is implemented over a large-scale network. We first observe that user dropouts of an FL network may lead to failure in achieving the desired level of privacy protection, i.e., over-consumption of the privacy budget. Subsequently, we develop a DP mechanism robust to user dropouts by dynamically calibrating noise with account of the dropout rate. We evaluate the proposed technique to train convolutional neural network models on MNIST and FEMNIST datasets over a simulated FL network. Our results show that our approach significantly improves privacy guarantee for user dropouts compared to existing DP algorithms on FL networks.</Abstract></Paper><Paper published="yes"><Idx>2</Idx><Title>On Data Licenses for Open Source Threat Intelligence</Title><Author>WooChul Shim (Samsung Reseasrch)</Author><Conference>IEEE Security &amp; Privacy</Conference><Year>2021</Year><Abstract>In this article, we discuss use restrictions in public threat data sources that hinder the growth of open source threat intelligence. We also investigate some misunderstandings about data usage rights and suggest directions for explicit licensing.</Abstract></Paper><Paper published="yes"><Idx>3</Idx><Title>Consistency Analysis of Data Usage Purposes in Mobile Apps</Title><Author>Duc Bui (University of Michigan)</Author><Conference>ACM Conference on Computer and Communications Security (ACM CCS) 2021</Conference><Year>2021</Year><Abstract>While privacy laws and regulations require apps and services to disclose the purposes of their data collection to the users (i.e., why do they collect my data?), the data usage in an app's actual behavior does not always comply with the purposes stated in its privacy policy. Automated techniques have been proposed to analyze apps' privacy policies and their execution behavior, but they often overlooked the purposes of the apps' data collection, use and sharing. To mitigate this oversight, we propose PurPliance, an automated system that detects the inconsistencies between the data-usage purposes stated in a natural language privacy policy and those of the actual execution behavior of an Android app. PurPliance analyzes the predicate-argument structure of policy sentences and classifies the extracted purpose clauses into a taxonomy of data purposes. Purposes of actual data usage are inferred from network data traffic. We propose a formal model to represent and verify the data usage purposes in the extracted privacy statements and data flows to detect policy contradictions in a privacy policy and flow-to-policy inconsistencies between network data flows and privacy statements. Our evaluation results of end-to-end contradiction detection have shown PurPliance to improve detection precision from 19% to 95% and recall from 10% to 50% compared to a state-of-the-art method. Our analysis of 23.1k Android apps has also shown PurPliance to detect contradictions in 18.14% of privacy policies and flow-to-policy inconsistencies in 69.66% of apps, indicating the prevalence of inconsistencies of data practices in mobile apps.</Abstract></Paper><Paper published="yes"><Idx>4</Idx><Title>Towards Usable and Secure Location-based Smartphone Authentication</Title><Author>Geumhwan Cho (Sungkyunkwan University)</Author><Conference>Symposium on Usable Privacy and Security (SOUPS) 2021</Conference><Year>2021</Year><Abstract>The concept of using location information to unlock smartphones is widely available on Android phones. To date, however, not much research has been conducted on investigating security and usability requirements for designing such location-based authentication services. To bridge this gap, we interviewed 18 participants, studying users' perceptions and identifying key design requirements such as the need to support fine-grained indoor location registration and location (unlock coverage) size adjustment. We then conducted a field study with 29 participants and a fully-functioning application to study real-world usage behaviors. On average, the participants were able to reduce about 36% of manual unlock attempts by using our application for three weeks. 28 participants enduringly used registered locations to unlock their phones despite being able to delete them during the study and unlock manually instead. Worryingly, however, 23 participants registered at least one insecure location - defined as a location where an unwanted adversary can physically access their phones - as a trusted location mainly due to convenience or low (perceived) likelihood of phones being attacked. 52 out of 65 total registered locations were classified as insecure by the definition above. Interestingly, regardless of whether locations were considered secure or insecure, the participants preferred to select large phone unlock coverage areas.</Abstract></Paper><Paper published="yes"><Idx>5</Idx><Title>#Twiti: Social Listening for Threat Intelligence</Title><Author>Hyejin Shin (Samsung Reseasrch)</Author><Conference>The Web Conference (WWW) 2021</Conference><Year>2021</Year><Abstract>Twitter is a popular public source for threat hunting. Many security vendors and security professionals use Twitter in practice for collecting Indicators of Compromise (IOCs). However, little is known about IOCs on Twitter. Their important characteristics such as earliness, uniqueness, and accuracy have never been investigated. Moreover, how to extract IOCs from Twitter with high accuracy is not obvious. In this paper, we present Twiti, a system that automatically extracts various forms of malware IOCs from Twitter. Based on the collected IOCs, we conduct the first empirical assessment and thorough analysis of malware IOCs on Twitter. Twiti extracts IOCs from tweets identified as having malware IOC information by leveraging natural language processing and machine learning techniques. With extensive evaluation, we demonstrate that not only can Twiti extract malware IOCs accurately, but also the extracted IOCs are unique and early. By analyzing IOCs in Twiti from various aspects, we find that Twitter captures ongoing malware threats such as Emotet variants and malware distribution sites better than other public threat intelligence (TI) feeds. We also find that only a tiny fraction of IOCs on Twitter come from commercial vendor accounts and individual Twitter users are the main contributors of the early detected or exclusive IOCs, which indicates that Twitter can provide many valuable IOCs uncovered in commercial domain.</Abstract></Paper><Paper published="yes"><Idx>6</Idx><Title>Constraint-guided Directed Greybox Fuzzing</Title><Author>Gwangmu Lee (SNU)</Author><Conference>USENIX Security Symposium 2021</Conference><Year>2021</Year><Abstract>Directed greybox fuzzing is an augmented fuzzing technique intended for the targeted usages such as crash reproduction and proof-of-concept generation, which gives directedness to fuzzing by driving the seeds toward the designated program locations called target sites. However, we find that directed greybox fuzzing can still suffer from the long fuzzing time before exposing the targeted crash, because it does not consider the ordered target sites and the data conditions. This paper presents constraint-guided directed greybox fuzzing that aims to satisfy a sequence of constraints rather than merely reaching a set of target sites. Constraint-guided greybox fuzzing defines a constraint as the combination of a target site and the data conditions, and drives the seeds to satisfy the constraints in the specified order. We automatically generate the constraints with seven types of crash dumps and four types of patch changelogs, and evaluate the prototype system CAFL against the representative directed greybox fuzzing system AFLGo with 47 real-world crashes and 12 patch changelogs. The evaluation shows CAFL outperforms AFLGo by 2.88x for crash reproduction, and better performs in PoC generation as the constraints get explicit.</Abstract></Paper><Paper published="yes"><Idx>7</Idx><Title>IronMask: Modular Architecture for Protecting Deep Face Template</Title><Author>Sunpill Kim (Hanyang University)</Author><Conference>Conference on Computer Vision and Pattern Recognition (CVPR) 2021</Conference><Year>2021</Year><Abstract>Convolutional neural networks have made remarkable progress in the face recognition field. The more the technology of face recognition advances, the greater discriminative features into a face template. However, this increases the threat to user privacy in case the template is exposed. In this paper, we present a modular architecture for face template protection, called IronMask, that can be combined with any face recognition system using angular distance metric. We circumvent the need for binarization, which is the main cause of performance degradation in most existing face template protections, by proposing a new real-valued error-correcting-code that is compatible with real-valued templates and can therefore, minimize performance degradation. We evaluate the efficacy of IronMask by extensive experiments on two face recognitions, ArcFace and CosFace with three datasets, CMU-Multi-PIE, FEI, and Color-FERET. According to our experimental results, IronMask achieves a true accept rate (TAR) of 99.79% at a false accept rate (FAR) of 0.0005% when combined with ArcFace, and 95.78% TAR at 0% FAR with CosFace, while providing at least 115-bit security against known attacks.</Abstract></Paper><Paper published="yes"><Idx>8</Idx><Title>On Smartphone Users Difficulty with Understanding Implicit Authentication</Title><Author>Masoud Mehrabi Koushki (University of British Columbia)</Author><Conference>ACM Conference on Human Factors in Computing Systems (ACM CHI) 2021</Conference><Year>2021</Year><Abstract>Implicit authentication (IA) has recently become a popular approach for providing physical security on smartphones. It relies on behavioral traits (e.g., gait patterns) for user identification, instead of biometric data or knowledge of a PIN. However, it is not yet known whether users can understand the semantics of this technology well enough to use it properly. We bridge this knowledge gap by evaluating how Android’s Smart Lock (SL), which is the first widely deployed IA solution on smartphones, is understood by its users. We conducted a qualitative user study (N=26) and an online survey (N=331). The results suggest that users often have difficulty understanding SL semantics, leaving them unable to judge when their phone would be (un)locked. We found that various aspects of SL, such as its capabilities and its authentication factors, are confusing for the users. We also found that depth of smartphone adoption is a significant antecedent of SL comprehension.</Abstract></Paper><Paper published="yes"><Idx>9</Idx><Title>Cybersecurity Event Detection with New and Re-emerging Words</Title><Author>Hyejin Shin (Samsung Reseasrch)</Author><Conference>ACM ASIA Conference on Computer and Communications Security (ACM ASIACCS 2020)</Conference><Year>2020</Year><Abstract>There is plenty of threat-related information in open data sources. Early identification of emerging security threats from such information is an important part of security for deployed software and systems. While several cybersecurity event detection methods have been proposed to extract security events from unstructured text in open data sources, most of the existing methods focus on detecting events that have a large volume of mentions. On the contrary, to respond faster than attackers, security analysts and IT operators need to be aware of critical security events as early as possible, no matter how many mentions about an event are made. In this paper, we propose a novel event detection system that can quickly identify critical security events, such as new threats and resurgence of an attack or related event, from Twitter regardless of their volume of mentions. Unlike the existing methods, the proposed method triggers events by monitoring new words and re-emerging words, making it possible to narrow down candidate events among several hundreds of events. It then forms events by clustering tweets linked with the trigger words. This approach enables us to detect new and resurgent threats as early as possible. We empirically demonstrate that our system works promisingly over a wide range of threat types.</Abstract></Paper><Paper published="yes"><Idx>10</Idx><Title>Exploiting Uses of Uninitialized Stack Variables in Linux Kernels to Leak Kernel Pointers</Title><Author>Haehyun Cho (Arizona State University)</Author><Conference>USENIX Workshop on Offensive Technologies (WOOT 2020)</Conference><Year>2020</Year><Abstract>Information leaks are the most prevalent type of vulnerabilities among all known vulnerabilities in Linux kernel. Many of them are caused by the use of uninitialized variables or data structures. It is generally believed that the majority of information leaks in Linux kernel are low-risk and do not have severe impact due to the difficulty (or even the impossibility) of exploitation. As a result, developers and security analysts do not pay enough attention to mitigating these vulnerabilities. Consequently, these vulnerabilities are usually assigned low CVSS scores or without any CVEs assigned. Moreover, many patches that address uninitialized data use bugs in Linux kernel are not accepted, leaving billions of Linux systems vulnerable. Nonetheless, information leak vulnerabilities in Linux kernel are not as low-risk as people believe. In this paper, we present a generic approach that converts stack-based information leaks in Linux kernel into kernel-pointer leaks, which can be used to defeat modern security defenses such as KASLR. Taking an exploit that triggers an information leak in Linux kernel, our approach automatically converts it into a highly impactful exploit that leaks pointers to either kernel functions or the kernel stack. We evaluate our approach on four known CVEs and one security patch in Linux kernel and demonstrate its effectiveness. Our findings provide solid evidence for Linux kernel developers and security analysts to treat information leaks in Linux kernel more seriously.</Abstract></Paper><Paper published="yes"><Idx>11</Idx><Title>Gesture Authentication for Smartphones: Evaluation of Gesture Password Selection Policies</Title><Author>Eunyong Cheon (UNIST)</Author><Conference>IEEE Symposium on Security and Privacy (IEEE S&amp;P 2020)</Conference><Year>2020</Year><Abstract>Touchscreen gestures are attracting research attention as an authentication method. While studies have showcased their usability, it has proven more complex to determine, let alone enhance, their security. Problems stem both from the small scale of current data sets and the fact that gestures are matched imprecisely - by a distance metric. This makes it challenging to assess entropy with traditional algorithms. To address these problems, we captured a large set of gesture passwords (N=2594) from crowd workers, and developed a security assessment framework that can calculate partial guessing entropy estimates, and generate dictionaries that crack 23.13% or more gestures in online attacks (within 20 guesses). To improve the entropy of gesture passwords, we designed novel blacklist and lexical policies to, respectively, restrict and inspire gesture creation. We close by validating both our security assessment framework and policies in a new crowd-sourced study (N=4000). Our blacklists increase entropy and resistance to dictionary based guessing attacks.</Abstract></Paper><Paper published="yes"><Idx>12</Idx><Title>SmokeBomb: effective mitigation against cache side-channel attacks on the ARM architecture</Title><Author>Haehyun Cho (Arizona State University)</Author><Conference>ACM International Conference on Mobile Systems, Applications, and Services (MobiSYS 2020)</Conference><Year>2020</Year><Abstract>Cache side-channel attacks abuse microarchitectural designs meant to optimize memory access to infer information about victim processes, threatening data privacy and security. Recently, the ARM architecture has come into the spotlight of cache side-channel attacks with its unprecedented growth in the market. We propose SmokeBomb, a novel cache side-channel mitigation that functions by explicitly ensuring a private space for each process to safely access sensitive data. The heart of the idea is to use the L1 cache of the CPU core as a private space by which SmokeBomb can give consistent results against cache attacks on the sensitive data, and thus, an attacker cannot distinguish specific data used by the victim. Our experimental results show that SmokeBomb can effectively prevent currently formalized cache attack methods.</Abstract></Paper><Paper published="yes"><Idx>13</Idx><Title>Void: A fast and light voice liveness detection system</Title><Author>Muhammad Ejaz Ahmed (Data61, CSIRO)</Author><Conference>USENIX Security Symposium 2020</Conference><Year>2020</Year><Abstract>Due to the open nature of voice assistants' input channels, adversaries could easily record people's use of voice commands, and replay them to spoof voice assistants. To mitigate such spoofing attacks, we present a highly efficient voice liveness detection solution called "Void." Void detects voice spoofing attacks using the differences in spectral power between live-human voices and voices replayed through speakers. In contrast to existing approaches that use multiple deep learning models, and thousands of features, Void uses a single classification model with just 97 features. We used two datasets to evaluate its performance: (1) 255,173 voice samples generated with 120 participants, 15 playback devices and 12 recording devices, and (2) 18,030 publicly available voice samples generated with 42 participants, 26 playback devices and 25 recording devices. Void achieves equal error rate of 0.3% and 11.6% in detecting voice replay attacks for each dataset, respectively. Compared to a state of the art, deep learning-based solution that achieves 7.4% error rate in that public dataset, Void uses 153 times less memory and is about 8 times faster in detection. When combined with a Gaussian Mixture Model that uses Mel-frequency cepstral coefficients (MFCC) as classification features—MFCC is already being extracted and used as the main feature in speech recognition services—Void achieves 8.7% error rate on the public dataset. Moreover, Void is resilient against hidden voice command, inaudible voice command, voice synthesis, equalization manipulation attacks, and combining replay attacks with live-human voices achieving about 99.7%, 100%, 90.2%, 86.3%, and 98.2% detection rates for those attacks, respectively.</Abstract></Paper><Paper published="yes"><Idx>14</Idx><Title>On the Security and Usability Implications of Multiple Authentication Choices</Title><Author>Geumhwan Cho (Sungkyunkwan Univ.)</Author><Conference>ACM Transactions on Privacy and Security</Conference><Year>2020</Year><Abstract>The latest smartphones have started providing multiple authentication options including PINs, patterns, and passwords (knowledge based), as well as face, fingerprint, iris, and voice identification (biometric-based). In this article, we conducted two user studies to investigate how the convenience and security of unlocking phones are influenced by the provision of multiple authentication options. In a task-based user study with 52 participants, we analyze how participants choose an option to unlock their smartphone in daily life. The user study results demonstrate that providing multiple biometric-based authentication choices does not really influence convenience, because fingerprint had monopolistic dominance in the usage of unlock methods (111 of a total of 115 unlock trials that used a biometric-based authentication factor) due to users’ habitual behavior and fastness in unlocking phones. However, convenience was influenced by the provision of both knowledge-based and biometric-based authentication categories, as biometric-based authentication options were used in combination with knowledge-based authentication options—pattern was another frequently used unlock method. Our findings were confirmed and generalized through a follow-up survey with 327 participants. First, knowledge-based and biometric-based authentication options are used interchangeably. Second, providing multiple authentication options for knowledge-based authentication may influence convenience—both PINs (55.7%) and patterns (39.2%) are quite evenly used. Last, in contrast to knowledge-based authentication, providing multiple authentication choices for biometric-based authentication has less influence on choosing unlock options—fingerprint scanner is the most frequently used option (134 of 187 unlock methods used among biometric-based authentication options).</Abstract></Paper><Paper published="yes"><Idx>15</Idx><Title>Enhancing Differential Privacy for Federated Learning</Title><Author>Chunghun Baek (Samsung Reseasrch)</Author><Conference>Samsung Best Paper Award 2020</Conference><Year>2020</Year><Abstract>(Forbidden)</Abstract></Paper><Paper published="yes"><Idx>16</Idx><Title>On smartphone users perception of smart lock for Android</Title><Author>Masoud Mehrabi Koushki (University of British Columbia)</Author><Conference>Conference on Mobile Human-Computer Interaction (Mobile HCI) 2020</Conference><Year>2020</Year><Abstract>Implicit authentication (IA) on smartphones has gained a lot of attention from the research community over the past decade. IA leverages behavioral and contextual data to identify users without requiring explicit input, and thus can alleviate the burden of smartphone unlocking. The reported studies on users’ perception of IA have painted a very positive picture, showing that more than 60% of their respective participants are interested in adopting IA, should it become available on their devices. These studies, however, have all been done either in lab environments, or with low- to medium-fidelity prototypes, which limits their generalizability and ecological validity. Therefore, the question of “how would smartphone users perceive a commercialized IA scheme in a realistic setting?” remains unanswered. To bridge this knowledge gap, we report on the findings of our qualitative user study (N = 26) and our online survey (N = 343) to understand how Android users perceive Smart Lock (SL). SL is the first and currently only widely-deployed IA scheme for smartphones. We found that SL is not a widely adopted technology, even among those who have an SL-enabled phone and are aware of the existence of the feature. Conversely, we found unclear usefulness, and perceived lack of security, among others, to be major adoption barriers that caused the SL adoption rate to be as low as 13%. To provide a theoretical framework for explaining SL adoption, we propose an extended version of the technology acceptance model (TAM), called SL-TAM, which sheds light on the importance of factors such as perceived security and utility on SL adoption.</Abstract></Paper><Paper published="yes"><Idx>17</Idx><Title>Learning New Words from Keystroke Data with Local Differential Privacy</Title><Author>Sungwook Kim (Samsung Reseasrch)</Author><Conference>IEEE Trans on Knowledge and Data Engineering (TKDE Volume: 32, Issue: 3, March 1 2020)</Conference><Year>2020</Year><Abstract>Keystroke data collected from smart devices includes various sensitive information about users. Collecting and analyzing such data raise serious privacy concerns. Google and Apple have recently applied local differential privacy (LDP) to address privacy issue on learning new words from users’ keystroke data. However, these solutions require multiple LDP reports for a single word, which result in inefficient use of privacy budget and high computational cost. In this paper, we develop a novel algorithm for learning new words under LDP. Unlike the existing solutions, the proposed method generates only one LDP report for a single word. This enables the proposed method to use full privacy budget for generating a report and brings the benefit that the proposed method provides better utility at the same privacy degree than the existing methods. In our algorithm, each user appends a hash value to new word and sends only one LDP report of an n-gram selected randomly from the string packed by each new word and its hash value. The server then decodes frequent n-grams at each position of the string and discovers the candidate words by exploring graph-theoretic links between n-grams and checking integrity of candidates with hash values. Frequencies of frequent new words discovered are estimated from distribution estimates of n-grams by robust regression. We theoretically show that our algorithm can recover popular new words even though the server does not know the domain of the raw data. In addition, we theoretically and empirically demonstrate that our algorithm achieves higher accuracy compared to the existing solutions.</Abstract></Paper><Paper published="yes"><Idx>18</Idx><Title>Collecting and Analyzing Multidimensional Data with Local Differential Privacy</Title><Author>Ning Wang (Samsung Reseasrch)</Author><Conference>IEEE International Conference on Data Engineering (IEEE ICDE 2019)</Conference><Year>2019</Year><Abstract>Local differential privacy (LDP) is a recently proposed privacy standard for collecting and analyzing data, which has been used, e.g., in the Chrome browser, iOS and macOS. In LDP, each user perturbs her information locally, and only sends the randomized version to an aggregator who performs analyses, which protects both the users and the aggregator against private information leaks. Although LDP has attracted much research attention in recent years, the majority of existing work focuses on applying LDP to complex data and/or analysis tasks. In this paper, we point out that the fundamental problem of collecting multidimensional data under LDP has not been addressed sufficiently, and there remains much room for improvement even for basic tasks such as computing the mean value over a single numeric attribute under LDP. Motivated by this, we first propose novel LDP mechanisms for collecting a numeric attribute, whose accuracy is at least no worse (and usually better) than existing solutions in terms of worst-case noise variance. Then, we extend these mechanisms to multidimensional data that can contain both numeric and categorical attributes, where our mechanisms always outperform existing solutions regarding worst-case noise variance. As a case study, we apply our solutions to build an LDP-compliant stochastic gradient descent algorithm (SGD), which powers many important machine learning tasks. Experiments using real datasets confirm the effectiveness of our methods, and their advantages over existing solutions.</Abstract></Paper><Paper published="yes"><Idx>19</Idx><Title>Users Really Do Answer Telephone Scams: Evaluating Attacks and Defenses</Title><Author>Huahong Tu (University of Maryland)</Author><Conference>USENIX Security Symposium 2019</Conference><Year>2019</Year><Abstract>As telephone scams become increasingly prevalent, it is crucial to understand what causes recipients to fall victim to these scams. Armed with this knowledge, effective countermeasures can be developed to challenge the key foundations of successful telephone phishing attacks. In this paper, we present the methodology, design, execution, results, and evaluation of an ethical telephone phishing scam. The study performed 10 telephone phishing experiments on 3,000 university participants without prior awareness over the course of a workweek. Overall, we were able to identify at least one key factor---spoofed Caller ID---that had a significant effect in tricking the victims into revealing their Social Security number.</Abstract></Paper><Paper published="yes"><Idx>20</Idx><Title>PhishFarm: A Scalable Framework for Measuring the Effectiveness of Evasion Techniques Against Browser Phishing Blacklists</Title><Author>Huahong Tu (University of Maryland)</Author><Conference>IEEE Symposium on Security and Privacy (IEEE S&amp;P 2019)</Conference><Year>2019</Year><Abstract>Phishing attacks have reached record volumes in recent years. Simultaneously, modern phishing websites are growing in sophistication by employing diverse cloaking techniques to avoid detection by security infrastructure. In this paper, we present PhishFarm: a scalable framework for methodically testing the resilience of anti-phishing entities and browser blacklists to attackers' evasion efforts. We use PhishFarm to deploy 2,380 live phishing sites (on new, unique, and previously-unseen .com domains) each using one of six different HTTP request filters based on real phishing kits. We reported subsets of these sites to 10 distinct anti-phishing entities and measured both the occurrence and timeliness of native blacklisting in major web browsers to gauge the effectiveness of protection ultimately extended to victim users and organizations. Our experiments revealed shortcomings in current infrastructure, which allows some phishing sites to go unnoticed by the security community while remaining accessible to victims. We found that simple cloaking techniques representative of real-world attacks- including those based on geolocation, device type, or JavaScript- were effective in reducing the likelihood of blacklisting by over 55% on average. We also discovered that blacklisting did not function as intended in popular mobile browsers (Chrome, Safari, and Firefox), which left users of these browsers particularly vulnerable to phishing attacks. Following disclosure of our findings, anti-phishing entities are now better able to detect and mitigate several cloaking techniques (including those that target mobile users), and blacklisting has also become more consistent between desktop and mobile platforms- but work remains to be done by anti-phishing entities to ensure users are adequately protected. Our PhishFarm framework is designed for continuous monitoring of the ecosystem and can be extended to test future state-of-the-art evasion techniques used by malicious websites.</Abstract></Paper><Paper published="yes"><Idx>21</Idx><Title>Voice Presentation Attack Detection through Text-Converted Voice Command Analysis</Title><Author>Il-youp Kwak (Samsung Reseasrch)</Author><Conference>ACM Conference on Human Factors in Computing Systems (ACM CHI 2019)</Conference><Year>2019</Year><Abstract>Voice assistants are quickly being upgraded to support advanced, security-critical commands such as unlocking devices, checking emails, and making payments. In this paper, we explore the feasibility of using users' text-converted voice command utterances as classification features to help identify users' genuine commands, and detect suspicious commands. To maintain high detection accuracy, our approach starts with a globally trained attack detection model (immediately available for new users), and gradually switches to a user-specific model tailored to the utterance patterns of a target user. To evaluate accuracy, we used a real-world voice assistant dataset consisting of about 34.6 million voice commands collected from 2.6 million users. Our evaluation results show that this approach is capable of achieving about 3.4% equal error rate (EER), detecting 95.7% of attacks when an optimal threshold value is used. As for those who frequently use security-critical (attack-like) commands, we still achieve EER below 5%.</Abstract></Paper><Paper published="yes"><Idx>22</Idx><Title>Voice Presentation Attack Detection through Text-Converted Voice Command Analysis</Title><Author>Seungtaek Han (Samsung Reseasrch)</Author><Conference>Samsung Best Paper Award 2019</Conference><Year>2019</Year><Abstract>(Forbidden)</Abstract></Paper><Paper published="yes"><Idx>23</Idx><Title>ACE: Automatic Cybersecurity Event Detector</Title><Author>Hyejin Shin (Samsung Reseasrch)</Author><Conference>Samsung Best Paper Award 2019</Conference><Year>2019</Year><Abstract>(Forbidden)</Abstract></Paper><Paper published="yes"><Idx>24</Idx><Title>FuzzBuilder: Automated building greybox fuzzing environment for C/C++ library</Title><Author>Joonun Jang (Samsung Research)</Author><Conference>Annual Computer Security Applications Conference (ACSAC 2019)</Conference><Year>2019</Year><Abstract>Fuzzing is an effective method to find bugs in software. Many security communities are interested in fuzzing as an automated approach to verify software security because most of the bugs discovered by fuzzing are related to security vulnerabilities. However, not all software can be tested by fuzzing because fuzzing requires a running environment, especially an executable. Notably, in the case of libraries, most of the libraries do not have a relevant executable in practice. Thus, state-of-the-art fuzzers have a limitation to test an arbitrary library. To overcome this problem, we propose FuzzBuilder to provide an automated fuzzing environment for libraries. FuzzBuilder generates an executable that calls library API functions to enable library fuzzing. Moreover, any executable generated by FuzzBuilder is compatible with existing fuzzers such as AFL. We evaluate the overall performance of FuzzBuilder by testing open source libraries. Consequently, we discovered unknown bugs in libraries while achieving high code coverage. We believe that FuzzBuilder helps security researchers to save both setup cost and learning cost for library fuzzing.</Abstract></Paper><Paper published="yes"><Idx>25</Idx><Title>Grey-box Concolic Testing on Binary Code</Title><Author>Jaeseung Choi (KAIST)</Author><Conference>International Conference on Software Engineering (ICSE 2019)</Conference><Year>2019</Year><Abstract>We present grey-box concolic testing, a novel path-based test case generation method that combines the best of both white-box and grey-box fuzzing. At a high level, our technique systematically explores execution paths of a program under test as in white-box fuzzing, a.k.a. concolic testing, while not giving up the simplicity of grey-box fuzzing: it only uses a lightweight instrumentation, and it does not rely on an SMT solver. We implemented our technique in a system called Eclipser, and compared it to the state-of-the-art grey-box fuzzers (including AFLFast, LAF-intel, Steelix, and VUzzer) as well as a symbolic executor (KLEE). In our experiments, we achieved higher code coverage and found more bugs than the other tools.</Abstract></Paper><Paper published="yes"><Idx>26</Idx><Title>Prime+Count: Novel Cross-world Covert Channels on ARM TrustZone</Title><Author>Haehyun Cho (Arizona State University)</Author><Conference>Annual Computer Security Applications Conference (AC-SAC 2018)</Conference><Year>2018</Year><Abstract>The security of ARM TrustZone relies on the idea of splitting system-on-chip hardware and software into two worlds, namely normal world and secure world. In this paper, we report cross-world covert channels, which exploit the world-shared cache in the TrustZone architecture. We design a Prime+Count technique that only cares about how many cache sets or lines have been occupied. The coarser-grained approach significantly reduces the noise introduced by the pseudo-random replacement policy and world switching. Using our Prime+Count technique, we build covert channels in single-core and cross-core scenarios in the TrustZone architecture. Our results demonstrate that Prime+Count is an effective technique for enabling cross-world covert channels on ARM TrustZone.</Abstract></Paper><Paper published="yes"><Idx>27</Idx><Title>PrivTrie: Effective Frequent Term Discovery under Local Differential Privacy</Title><Author>Ning Wang (Samsung Research)</Author><Conference>IEEE International Conference on Data Engineering (ICDE 2018)</Conference><Year>2018</Year><Abstract>A mobile operating system often needs to collect frequent new terms from users in order to build and maintain a comprehensive dictionary. Collecting keyboard usage data, however, raises privacy concerns. Local differential privacy (LDP) has been established as a strong privacy standard for collecting sensitive information from users. Currently, the best known solution for LDP-compliant frequent term discovery transforms the problem into collecting n-grams under LDP, and subsequently reconstructs terms from the collected n-grams by modelling the latter into a graph, and identifying cliques on this graph. Because the transformed problem (i.e., collecting n-grams) is very different from the original one (discovering frequent terms), the end result has poor utility. Further, this method is also rather expensive due to clique computation on a large graph. In this paper we tackle the problem head on: our proposal, PrivTrie, directly collects frequent terms from users by iteratively constructing a trie under LDP. While the methodology of building a trie is an obvious choice, obtaining an accurate trie under LDP is highly challenging. PrivTrie achieves this with a novel adaptive approach that conserves privacy budget by building internal nodes of the trie with the lowest level of accuracy necessary. Experiments using real datasets confirm that PrivTrie achieves high accuracy on common privacy levels, and consistently outperforms all previous methods.</Abstract></Paper><Paper published="yes"><Idx>28</Idx><Title>Privacy Enhanced Matrix Factorization for Recommendation with Local Differential Privacy</Title><Author>Hyejin Shin (Samsung Research)</Author><Conference>IEEE Trans on Knowledge and Data Engineering (TKDE Volume: 30, Issue: 9, Sept. 1 2018)</Conference><Year>2018</Year><Abstract>Recommender systems are collecting and analyzing user data to provide better user experience. However, several privacy concerns have been raised when a recommender knows user's set of items or their ratings. A number of solutions have been suggested to improve privacy of legacy recommender systems, but the existing solutions in the literature can protect either items or ratings only. In this paper, we propose a recommender system that protects both user's items and ratings. For this, we develop novel matrix factorization algorithms under local differential privacy (LDP). In a recommender system with LDP, individual users randomize their data themselves to satisfy differential privacy and send the perturbed data to the recommender. Then, the recommender computes aggregates of the perturbed data. This framework ensures that both user's items and ratings remain private from the recommender. However, applying LDP to matrix factorization typically raises utility issues with i) high dimensionality due to a large number of items and ii) iterative estimation algorithms. To tackle these technical challenges, we adopt dimensionality reduction technique and a novel binary mechanism based on sampling. We additionally introduce a factor that stabilizes the perturbed gradients. With MovieLens and LibimSeTi datasets, we evaluate recommendation accuracy of our recommender system and demonstrate that our algorithm performs better than the existing differentially private gradient descent algorithm for matrix factorization under stronger privacy requirements.</Abstract></Paper><Paper published="yes"><Idx>29</Idx><Title>I'm Listening to your Location! Inferring User Location with Acoustic Side Channels.</Title><Author>Youngbae Jeon (Korea University)</Author><Conference>The Web Conference (WWW) 2018</Conference><Year>2018</Year><Abstract>Electrical network frequency (ENF) signals have common patterns that can be used as signatures for identifying recorded time and location of videos and sound. To enable cost-efficient, reliable and scalable location inference, we created a reference map of ENF signals representing hundreds of locations world wide -- extracting real-world ENF signals from online multimedia streaming services (e.g., YouTube and Explore). Based on this reference map of ENF signals, we propose a novel side-channel attack that can identify the physical location of where a target video or sound was recorded or streamed from. Our attack does not require any expensive ENF signal receiver nor any software to be installed on a victim»s device -- all we need is the recorded video or sound files to perform the attack and they are collected from world wide web. The evaluation results show that our attack can infer the intra-grid location of the recorded audio files with an accuracy of $76$% when those files are $5$ minutes or longer. We also showed that our proposed attack works well even when video and audio data are processed within a certain distortion range with audio codecs used in real VoIP applications.</Abstract></Paper><Paper published="yes"><Idx>30</Idx><Title>The Personal Identification Chord: A Four Button Authentication System for Smartwatches</Title><Author>Ian Oakley (Samsung Research)</Author><Conference>ACM ASIA Conference on Computer and Communications Security (ASIACCS 2018)</Conference><Year>2018</Year><Abstract>Smartwatches support access to a wide range of private information but little is known about the security and usability of existing smartwatch screen lock mechanisms. Prior studies suggest that smartwatch authentication via standard techniques such as 4-digit PINs is challenging and error-prone. We conducted interviews to shed light on current practices, revealing that smartwatch users consider the ten-key keypad required for PIN entry to be hard to use due to its small button sizes. To address this issue, we propose the Personal Identification Chord (PIC), an authentication system based on a four-button chorded keypad that enables users to enter ten different inputs via taps to one or two larger buttons. Two studies assessing usability and security of our technique indicate PICs lead to increases in setup and (modestly) recall time, but can be entered accurately while maintaining high recall rates and may improve guessing entropy compared to PINs.</Abstract></Paper><Paper published="yes"><Idx>31</Idx><Title>Efficient Privacy-Preserving Matrix Factorization for Recommendation via Fully Homomorphic Encryption</Title><Author>JINSU KIM (Samsung Research)</Author><Conference>ACM Transactions on Privacy and Security (Volume 21 Issue 4, June 2018))</Conference><Year>2018</Year><Abstract>There are recommendation systems everywhere in our daily life. The collection of personal data of users by a recommender in the system may cause serious privacy issues. In this article, we propose the first privacy-preserving matrix factorization for recommendation using fully homomorphic encryption. Our protocol performs matrix factorization over encrypted users’ rating data and returns encrypted outputs so that the recommendation system learns nothing on rating values and resulting user/item profiles. Furthermore, the protocol provides a privacy-preserving method to optimize the tuning parameters that can be a business benefit for the recommendation service providers. To overcome the performance degradation caused by the use of fully homomorphic encryption, we introduce a novel data structure to perform computations over encrypted vectors, which are essential for matrix factorization, through secure two-party computation in part. Our experiments demonstrate the efficiency of our protocol.</Abstract></Paper><Paper published="no"><Idx>32</Idx><Title>KingWangZzang: A super ultra great AEG(Automatic Exploit Generator) for ethical hackers.</Title><Author>Matta Park (Samsung Research)</Author><Conference>Samsung Security Wish Forum</Conference><Year>2021</Year><Abstract>In this paper, we propose KingWangZzang, an AEG(automatic exploit generator) which works for all modern computer platforms. By using KingWangZzang, we can achieve world peace, hopefuuuuuuuly. Unfortunately, we cannot reveal KingWangZzang's internal structure, but this hint will help you: SCTF{W4KE_up_IT's_mOndAy_m0rn1n9_183689c7}</Abstract></Paper></Papers>
